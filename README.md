# Team-04
Team 4  Document summary validation tool
"# MCI-Project-University-Course--My-Contribution" 

Overview: This repository records the development contribution I made for this team. Contribution of writing reports is not redorded here. In development, I am responsible to writing the whole back-end server, and design and maintain the cloud Database.

Project requirement:
SDS and MDS share the same human evaluation metrics. Just MDS need to consider multiple source documents when evaluating.
There are four summaries in the attachment. The summaries generated by four models are the txt files named "text_[Model_Name].txt" (e.g., text_himap.txt)
The "source_documents.txt" file contains the source documents which are the inputs of the models.
There are 10 examples to be evaluated, and thus there are 10 lines in each txt file, while each line is a source document or summary. Besides, they are sorted, which means the same line in different files are in the same context.
For MDS, "story_separator_special_tag" is a special token to separate documents in a document set. For example: doc 1 story_separator_special_tag doc2 story_separator_special_tag doc 3.
The score could range from 1-5 (1 means very bad, 5 means very good). 
For human evaluation, we have 3 aspects: fluency, informativeness, and non-redundancy. 
Fluency: whether the summary is both both syntactically and semantically correct, natural and well-formed.
Informativeness: whether the summary keeps the important information from the source documents. Besides, for multiple documents summarization, the summary is also supposed to have salient information from different sources, instead of single one.
Non-redundancy: whether the summary contains some information that are repeated or useless.
